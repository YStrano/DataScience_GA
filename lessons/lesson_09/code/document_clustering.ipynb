{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering text documents using k-means\n",
    "\n",
    "\n",
    "This is an example showing how the scikit-learn can be used to cluster\n",
    "documents by topics using a bag-of-words approach. This example uses\n",
    "a scipy.sparse matrix to store the features instead of standard numpy arrays.\n",
    "\n",
    "Two feature extraction methods can be used in this example:\n",
    "\n",
    "  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n",
    "    frequent words to features indices and hence compute a word occurrence\n",
    "    frequency (sparse) matrix. The word frequencies are then reweighted using\n",
    "    the Inverse Document Frequency (IDF) vector collected feature-wise over\n",
    "    the corpus.\n",
    "\n",
    "  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n",
    "    possibly with collisions. The word count vectors are then normalized to\n",
    "    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n",
    "    seems to be important for k-means to work in high dimensional space.\n",
    "\n",
    "    HashingVectorizer does not provide IDF weighting as this is a stateless\n",
    "    model (the fit method does nothing). When IDF weighting is needed it can\n",
    "    be added by pipelining its output to a TfidfTransformer instance.\n",
    "\n",
    "Two algorithms are demoed: ordinary k-means and its more scalable cousin\n",
    "minibatch k-means.\n",
    "\n",
    "Additionally, latent semantic analysis can also be used to reduce dimensionality\n",
    "and discover latent patterns in the data.\n",
    "\n",
    "It can be noted that k-means (and minibatch k-means) are very sensitive to\n",
    "feature scaling and that in this case the IDF weighting helps improve the\n",
    "quality of the clustering by quite a lot as measured against the \"ground truth\"\n",
    "provided by the class label assignments of the 20 newsgroups dataset.\n",
    "\n",
    "This improvement is not visible in the Silhouette Coefficient which is small\n",
    "for both as this measure seem to suffer from the phenomenon called\n",
    "\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\n",
    "datasets such as text data. Other measures such as V-measure and Adjusted Rand\n",
    "Index are information theoretic based evaluation scores: as they are only based\n",
    "on cluster assignments rather than distances, hence not affected by the curse\n",
    "of dimensionality.\n",
    "\n",
    "Note: as k-means is optimizing a non-convex objective function, it will likely\n",
    "end up in a local optimum. Several runs with independent random init might be\n",
    "necessary to get a good convergence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387 documents\n",
      "4 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True number of Clusters:  4\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "print(\"True number of Clusters: \", true_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: healta@saturn.wwc.edu (Tammy R Healy)',\n",
       " 'Subject: Re: who are we to judge, Bobby?',\n",
       " 'Lines: 38',\n",
       " 'Organization: Walla Walla College',\n",
       " 'Lines: 38',\n",
       " '',\n",
       " 'In article <1993Apr14.213356.22176@ultb.isc.rit.edu> snm6394@ultb.isc.rit.edu (S.N. Mozumder ) writes:',\n",
       " '>From: snm6394@ultb.isc.rit.edu (S.N. Mozumder )',\n",
       " '>Subject: Re: who are we to judge, Bobby?',\n",
       " '>Date: Wed, 14 Apr 1993 21:33:56 GMT',\n",
       " '>In article <healta.56.734556346@saturn.wwc.edu> healta@saturn.wwc.edu (TAMMY R HEALY) writes:',\n",
       " '>>Bobby,',\n",
       " '>>',\n",
       " '>>I would like to take the liberty to quote from a Christian writer named ',\n",
       " '>>Ellen G. White.  I hope that what she said will help you to edit your ',\n",
       " '>>remarks in this group in the future.',\n",
       " '>>',\n",
       " '>>\"Do not set yourself as a standard.  Do not make your opinions, your views ',\n",
       " '>>of duty, your interpretations of scripture, a criterion for others and in ',\n",
       " '>>your heart condemn them if they do not come up to your ideal.\"',\n",
       " '>>                         Thoughts Fromthe Mount of Blessing p. 124',\n",
       " '>>',\n",
       " \">>I hope quoting this doesn't make the atheists gag, but I think Ellen White \",\n",
       " '>>put it better than I could.',\n",
       " '>> ',\n",
       " '>>Tammy',\n",
       " '>',\n",
       " '>Point?',\n",
       " '>',\n",
       " '>Peace,',\n",
       " '>',\n",
       " '>Bobby Mozumder',\n",
       " '>',\n",
       " 'My point is that you set up your views as the only way to believe.  Saying ',\n",
       " 'that all eveil in this world is caused by atheism is ridiculous and ',\n",
       " 'counterproductive to dialogue in this newsgroups.  I see in your posts a ',\n",
       " \"spirit of condemnation of the atheists in this newsgroup bacause they don'\",\n",
       " \"t believe exactly as you do.  If you're here to try to convert the atheists \",\n",
       " \"here, you're failing miserably.  Who wants to be in position of constantly \",\n",
       " 'defending themselves agaist insulting attacks, like you seem to like to do?!',\n",
       " \"I'm sorry you're so blind that you didn't get the messgae in the quote, \",\n",
       " 'everyone else has seemed to.',\n",
       " '',\n",
       " 'Tammy',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: tmc@spartan.ac.BrockU.CA (Tim Ciceran)',\n",
       " 'Subject: Re: Best FTP Viewer please.',\n",
       " 'Organization: Brock University, St. Catharines Ontario',\n",
       " 'X-Newsreader: TIN [version 1.1 PL9]',\n",
       " 'Lines: 19',\n",
       " '',\n",
       " 'SITUNAYA@IBM3090.BHAM.AC.UK wrote:',\n",
       " ': ==============================================================================',\n",
       " \": Could someone please tell me the Best FTP'able viewer available for MSDOS\",\n",
       " ': I am running a 486 33mhz with SVGA monitor.',\n",
       " ': I need to look at gifs mainly and it would be advantageous if it ran',\n",
       " ': under windows...........thanks',\n",
       " '',\n",
       " 'FTP to wuarchive.wustl.edu,',\n",
       " 'change into mirrors/msdos/graphics',\n",
       " 'get \"grfwk61t.zip\"',\n",
       " 'This is the DOS version of Graphic Workshop.  There is a Windows version which',\n",
       " \"you could probably find in the mirrors/msdos/windows3 directory but I don't \",\n",
       " 'know what the file name is. ',\n",
       " '',\n",
       " '-- ',\n",
       " '',\n",
       " 'TMC',\n",
       " '(tmc@spartan.ac.BrockU.ca)',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[500].split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Text data to Numeric using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a TFidF vectorizer\n",
      "done in 3.653120s\n",
      "n_samples: 3387, n_features: 361840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from the training dataset using a TFidF vectorizer\")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 361840)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
      "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "done in 161.488s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "km = KMeans(n_clusters=true_k, max_iter = 1000)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Silhouette Score and provide some other metrics for you to investigate on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.478\n",
      "Completeness: 0.549\n",
      "V-measure: 0.511\n",
      "Adjusted Rand-Index: 0.423\n",
      "Silhouette Coefficient: 0.004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at some of the predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observation num.</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Observation num.  Cluster\n",
       "0                 0        0\n",
       "1                 1        3\n",
       "2                 2        3\n",
       "3                 3        3\n",
       "4                 4        3\n",
       "5                 5        3\n",
       "6                 6        3\n",
       "7                 7        1\n",
       "8                 8        1\n",
       "9                 9        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(range(10),km.predict(X)[:10])), columns = ['Observation num.', 'Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: JDB1145@tamvm1.tamu.edu',\n",
       " 'Subject: Re: A Little Too Satanic',\n",
       " 'Organization: Texas A&M University',\n",
       " 'Lines: 21',\n",
       " 'NNTP-Posting-Host: tamvm1.tamu.edu',\n",
       " '',\n",
       " 'In article <65934@mimsy.umd.edu>',\n",
       " 'mangoe@cs.umd.edu (Charley Wingate) writes:',\n",
       " ' ',\n",
       " '>',\n",
       " '>Nanci Ann Miller writes:',\n",
       " '>',\n",
       " ']The \"corrupted over and over\" theory is pretty weak.  Comparison of the',\n",
       " ']current hebrew text with old versions and translations shows that the text',\n",
       " ']has in fact changed very little over a space of some two millennia.  This',\n",
       " \"]shouldn't be all that suprising; people who believe in a text in this manner\",\n",
       " ']are likely to makes some pains to make good copies.',\n",
       " ' ',\n",
       " 'Tell it to King James, mate.',\n",
       " ' ',\n",
       " ']C. Wingate        + \"The peace of God, it is no peace,',\n",
       " ']                  +    but strife closed in the sod.',\n",
       " ']mangoe@cs.umd.edu +  Yet, brothers, pray for but one thing:',\n",
       " ']tove!mangoe       +    the marv\\'lous peace of God.\"',\n",
       " ' ',\n",
       " ' ',\n",
       " 'John Burke, jdb1145@summa.tamu.edu',\n",
       " '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[9].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: healta@saturn.wwc.edu (Tammy R Healy)',\n",
       " 'Subject: Re: who are we to judge, Bobby?',\n",
       " 'Lines: 38',\n",
       " 'Organization: Walla Walla College',\n",
       " 'Lines: 38',\n",
       " '',\n",
       " 'In article <1993Apr14.213356.22176@ultb.isc.rit.edu> snm6394@ultb.isc.rit.edu (S.N. Mozumder ) writes:',\n",
       " '>From: snm6394@ultb.isc.rit.edu (S.N. Mozumder )',\n",
       " '>Subject: Re: who are we to judge, Bobby?',\n",
       " '>Date: Wed, 14 Apr 1993 21:33:56 GMT',\n",
       " '>In article <healta.56.734556346@saturn.wwc.edu> healta@saturn.wwc.edu (TAMMY R HEALY) writes:',\n",
       " '>>Bobby,',\n",
       " '>>',\n",
       " '>>I would like to take the liberty to quote from a Christian writer named ',\n",
       " '>>Ellen G. White.  I hope that what she said will help you to edit your ',\n",
       " '>>remarks in this group in the future.',\n",
       " '>>',\n",
       " '>>\"Do not set yourself as a standard.  Do not make your opinions, your views ',\n",
       " '>>of duty, your interpretations of scripture, a criterion for others and in ',\n",
       " '>>your heart condemn them if they do not come up to your ideal.\"',\n",
       " '>>                         Thoughts Fromthe Mount of Blessing p. 124',\n",
       " '>>',\n",
       " \">>I hope quoting this doesn't make the atheists gag, but I think Ellen White \",\n",
       " '>>put it better than I could.',\n",
       " '>> ',\n",
       " '>>Tammy',\n",
       " '>',\n",
       " '>Point?',\n",
       " '>',\n",
       " '>Peace,',\n",
       " '>',\n",
       " '>Bobby Mozumder',\n",
       " '>',\n",
       " 'My point is that you set up your views as the only way to believe.  Saying ',\n",
       " 'that all eveil in this world is caused by atheism is ridiculous and ',\n",
       " 'counterproductive to dialogue in this newsgroups.  I see in your posts a ',\n",
       " \"spirit of condemnation of the atheists in this newsgroup bacause they don'\",\n",
       " \"t believe exactly as you do.  If you're here to try to convert the atheists \",\n",
       " \"here, you're failing miserably.  Who wants to be in position of constantly \",\n",
       " 'defending themselves agaist insulting attacks, like you seem to like to do?!',\n",
       " \"I'm sorry you're so blind that you didn't get the messgae in the quote, \",\n",
       " 'everyone else has seemed to.',\n",
       " '',\n",
       " 'Tammy',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: 18084TM@msu.edu (Tom)',\n",
       " 'Subject: Moonbase race',\n",
       " 'X-Added: Forwarded by Space Digest',\n",
       " 'Organization: [via International Space University]',\n",
       " 'Original-Sender: isu@VACATION.VENARI.CS.CMU.EDU',\n",
       " 'Distribution: sci',\n",
       " 'Lines: 22',\n",
       " '',\n",
       " 'George William Herbert sez:',\n",
       " '',\n",
       " '>Hmm.  $1 billion, lesse... I can probably launch 100 tons to LEO at',\n",
       " '>$200 million, in five years, which gives about 20 tons to the lunar',\n",
       " '>surface one-way.  Say five tons of that is a return vehicle and its',\n",
       " '>fuel, a bigger Mercury or something (might get that as low as two',\n",
       " \">tons), leaving fifteen tons for a one-man habitat and a year's supplies?\",\n",
       " '>Gee, with that sort of mass margins I can build the systems off',\n",
       " '>the shelf for about another hundred million tops.  That leaves',\n",
       " \">about $700 million profit.  I like this idea 8-)  Let's see\",\n",
       " '>if you guys can push someone to make it happen 8-) 8-)',\n",
       " '',\n",
       " \"I like your optimism, George.  I don't know doots about raising that kind\",\n",
       " 'of dough, but if you need people to split the work and the $700M, you just',\n",
       " 'give me a ring :-)  Living alone for a year on the moon sounds horrid, but',\n",
       " \"I'd even try that, if I got a bigger cut.  :-)\",\n",
       " '',\n",
       " '-Tommy Mac',\n",
       " '-------------------------------------------------------------------------',\n",
       " 'Tom McWilliams 517-355-2178 wk   \\\\\\\\ As the radius of vision increases,',\n",
       " '18084tm@ibm.cl.msu.edu 336-9591 hm \\\\\\\\ the circumference of mystery grows.',\n",
       " '-------------------------------------------------------------------------',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[7].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)',\n",
       " \"Subject: JPL's VLBI Project Meets with International Space Agencies\",\n",
       " 'Organization: Jet Propulsion Laboratory',\n",
       " 'Lines: 112',\n",
       " 'Distribution: world',\n",
       " 'NNTP-Posting-Host: kelvin.jpl.nasa.gov',\n",
       " 'Keywords: VLBI, JPL',\n",
       " 'News-Software: VAX/VMS VNEWS 1.41    ',\n",
       " '',\n",
       " 'From the \"JPL Universe\"',\n",
       " 'April 23, 1993',\n",
       " '',\n",
       " 'VLBI project meets with international space agencies',\n",
       " '',\n",
       " 'By Ed McNevin',\n",
       " \"     Members of JPL's Space Very Long Baseline Interferometry\",\n",
       " '(VLBI) project team recently concluded a week-long series of',\n",
       " 'meetings with officials from Russia and Japan.',\n",
       " '     The meetings were part of \"Space VLBI Week\" held at JPL in',\n",
       " 'early March and were intended to maintain cooperation between',\n",
       " 'international space agencies participating in the development of',\n",
       " 'the U.S. Space VLBI Project, a recently approved JPL flight',\n",
       " 'project set for launch in 1995.',\n",
       " '     U.S. Space VLBI will utilize two Earth-orbiting spacecraft',\n",
       " '-- the Japanese VSOP (VLBI Space Observing Program) satellite',\n",
       " 'with its 8-meter radio telescope, and a Russian RADIOASTRON',\n",
       " '10-meter satellite. Both spacecraft will team up with',\n",
       " 'ground-based radio telescopes located around the world to create',\n",
       " 'a radio telescope network that astronomers hope will expand radio',\n",
       " 'telescope observing power by a factor of 10.',\n",
       " \"     Japan's VSOP satellite will use a limited six-hour orbit to\",\n",
       " 'conduct imaging science, while the Russian RADIOASTRON spacecraft',\n",
       " 'will exploit a larger, 28-hour Earth orbit to conduct exploratory',\n",
       " 'radio astronomy. Each satellite will point at a source target for',\n",
       " 'roughly 24 hours, while approximately 20 ground-based radio',\n",
       " 'telescopes will simultaneously point at the same source object',\n",
       " 'while within view on Earth.',\n",
       " \"     According to Dr. Joel Smith, JPL's project manager for the\",\n",
       " 'U.S. Space VLBI, meetings like those held at JPL will permit',\n",
       " 'Japan and Russia, who have little previous experience in radio',\n",
       " 'interferometry, to establish working relationships with the radio',\n",
       " 'astronomy communities that will be vital during the complex',\n",
       " 'observations required by the Space VLBI project.',\n",
       " '     \"One of our main activities is developing the methodology',\n",
       " 'for international coordination, because the two spacecraft',\n",
       " 'simultaneously rely on the corresponding tracking stations while',\n",
       " 'using the ground-based radio telescopes to observe the same',\n",
       " 'celestial objects,\" said Smith.',\n",
       " '     Three new tracking antennas are being built at DSN',\n",
       " 'facilities and other three other tracking facilities located in',\n",
       " 'Japan, Russia and Green Bank, W.Va. This global network of',\n",
       " 'ground-based radio telescopes will use precision clocks and',\n",
       " 'high-speed recorders to collect observation data and forward the',\n",
       " 'information to a correlator located at the National Radio',\n",
       " 'Astronomy Observatory in Socorro, N.M. The correlator will',\n",
       " 'combine and process data, then make it available to mission',\n",
       " 'investigators in Moscow, Tokyo, and JPL via electronic mail.',\n",
       " '     Smith is optimistic that the massive radio telescope created',\n",
       " 'by the Space VLBI network will provide radio astronomers with',\n",
       " 'better resolution than has ever been achieved before by',\n",
       " 'ground-based radio telescopes, allowing astronomers to take a',\n",
       " 'closer look at distant objects in space.',\n",
       " '     \"There is a long history of radio astronomy using',\n",
       " 'ground-based telescopes,\" said Smith. \"What we intend to do is to',\n",
       " 'extend radio astronomy into Earth orbit. Our goal is to look',\n",
       " 'deeper into the cores of galactic nuclei, quasars and other',\n",
       " 'active radio sources to understand what drives those things we',\n",
       " 'have seen so far with radio astronomy.\"',\n",
       " '     Smith noted that if one examines \"the active galactic',\n",
       " \"nuclei, you'll find jets appearing to spew at speeds greater than\",\n",
       " 'light, and at energy levels that are millions of times greater',\n",
       " 'than you would expect.\"',\n",
       " '     He said some astronomers believe that black holes may be',\n",
       " 'located in the cores of these galaxies, and that they may fuel',\n",
       " 'the jets. Smith hopes that \"by using Space VLBI to look further',\n",
       " 'into the cores, this theory may be supported or disproved.\"',\n",
       " '     Russian space-flight hardware, including transponders and',\n",
       " 'transmitters, are now being tested in the United States, and',\n",
       " 'Japanese hardware is scheduled to arrive for testing later this',\n",
       " 'year. Analysis of this hardware will permit U.S. scientists and',\n",
       " 'engineers to understand how to modify the high-speed VLBA',\n",
       " 'Correlator operating at the NRAO in order to accommodate the odd',\n",
       " 'data patterns that will originate from the more than 20',\n",
       " 'ground-based radio telescopes involved in Space VLBI.',\n",
       " '     Smith is particularly pleased that meetings with the',\n",
       " 'Japanese and Russian space agency officials -- like those held at',\n",
       " 'JPL in March -- have proceeded smoothly. Yet he knows that the',\n",
       " \"political uncertainty in Russia could jeopardize that country's\",\n",
       " 'participation in the project.',\n",
       " '     \"Nothing is ever smooth,\" he said, \"but the Russians have',\n",
       " 'been incredibly open with us. We always anticipated some',\n",
       " 'likelihood that we will not succeed because of political factors',\n",
       " 'beyond our control, yet there tends to be a way of keeping these',\n",
       " 'things going, because scientists on both sides are trying hard,',\n",
       " 'and people recognize the value of cooperation at this level.\"',\n",
       " '     Smith points out that the Japanese space agency has more at',\n",
       " 'stake than just fulfilling an international commitment to a',\n",
       " 'science mission.',\n",
       " '     \"The Japanese have been extremely cooperative, since',\n",
       " 'international cooperation is essential to their science mission,\"',\n",
       " 'he said.',\n",
       " '     But Smith also noted that Japanese space agency officials',\n",
       " 'look at the U.S. Space VLBI mission as an opportunity to showcase',\n",
       " 'the technology involved with VSOP spacecraft, and their highly',\n",
       " 'regarded Mach V launch vehicle.',\n",
       " '     Yet regardless of the risks involved in undertaking such an',\n",
       " \"ambitious project, JPL's Smith is satisfied that planning for the\",\n",
       " 'Space VLBI Project is beyond the significant financial and',\n",
       " 'political hurdles that otherwise might threaten the project.',\n",
       " '     \"Fortunately, we have the virtue of having two partners, and',\n",
       " 'if either falls out, we would still have something with the',\n",
       " 'other. By themselves, both spacecraft are independent,',\n",
       " 'scientifically exciting missions.\"',\n",
       " '                           ###',\n",
       " '     ___    _____     ___',\n",
       " '    /_ /|  /____/ \\\\  /_ /|     Ron Baalke         | baalke@kelvin.jpl.nasa.gov',\n",
       " '    | | | |  __ \\\\ /| | | |     Jet Propulsion Lab |',\n",
       " ' ___| | | | |__) |/  | | |__   M/S 525-3684 Telos | The aweto from New Zealand',\n",
       " '/___| | | |  ___/    | |/__ /| Pasadena, CA 91109 | is part caterpillar and',\n",
       " '|_____|/  |_|/       |_____|/                     | part vegetable.',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[8].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
